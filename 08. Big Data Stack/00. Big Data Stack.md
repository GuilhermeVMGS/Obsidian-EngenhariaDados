
Uma Big Data Stack é o conjunto de ferramentas e tecnologias que trabalham juntas para gerenciar todo o ciclo de vida do dado desde o momento em que ele é gerado até a hora que vira uma informação.

As 4 camadas da Big Data Stack:

1. **Ingestão (Coleta):** são as ferramentas que "sugam" os dados das fontes (APIs, logs, bancos de dados de sites) e jogam para dentro do sistema. *Ferramentas:* Apache Kafka, Airbyte, Fivetran...
2. **Armazenamento (Onde o dado mora):** onde guardamos os volumes massivos de dados de forma barata. *Ferramentas:* AWS S3, Azure Data Lakse Storage...
3. **Processamento e Transformação:** onde o dado bruto (bronze) é limpo em transformado em dado útil (gold). É aqui que entra o poder computacional. *Ferramentas:* Apache Spark...
4. **Análise e Visualização:** a camada final, onde os dados já tratados são consumidos. *Ferramentas:* PBI, Tableau...