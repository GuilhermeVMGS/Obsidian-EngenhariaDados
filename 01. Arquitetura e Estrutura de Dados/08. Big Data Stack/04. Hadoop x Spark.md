
Tanto o Hadoop quanto o Spark são sistemas distribuídos que permitem processar dados em escala. Eles poderão se recuperar de uma falha se o processamento de dados for interrompido por qualquer motivo.

**Tolerância a Falhas:**

O **Apache Hadoop** continua a ser executado mesmo se um ou vários nós de processamento de dados falham. Ele faz várias cópias do mesmo bloco de dados e as armazena em vários nós. Quando um nó falha, o Hadoop recupera as informações de outro nó e as prepara para o processamento de dados.

Enquanto isso, o **Apache Spark** conta com uma tecnologia especial de processamento de dados chamada _Conjunto de dados distribuídos resiliente_ (RDD). Com o RDD, o Apache Spark lembra como ele recupera informações específicas do armazenamento e pode reconstruir os dados se o armazenamento subjacente falha.

**Arquitetura** 

O Hadoop tem um sistema de arquivos nativo chamado Sistema de Arquivos Distribuído do Hadoop (HDFS). O HDFS permite que o Hadoop divida grandes blocos de dados em vários blocos menores e uniformes. Em seguida, ele armazena os pequenos blocos de dados em grupos de servidores.

Enquanto isso, o Apache Spark não tem seu próprio sistema de arquivos nativo. Muitas organizações usam o Spark no sistema de arquivos do Hadoop para armazenar, gerenciar e recuperar dados.

**Performance**

O Hadoop pode processar grandes conjuntos de dados em lotes, mas pode ser mais lento. Para processar dados, o Hadoop lê as informações do armazenamento externo e, em seguida, analisa e insere os dados em algoritmos de software.

Para cada etapa do processamento de dados, o Hadoop grava os dados de volta no armazenamento externo, o que aumenta a latência. Portanto, não é adequado para tarefas de processamento em tempo real, mas é ideal para workloads com atrasos toleráveis. Por exemplo, o Hadoop é adequado para analisar registros mensais de vendas. Mas pode não ser a melhor opção para determinar o sentimento da marca em tempo real de feeds de mídias sociais. 

O Apache Spark, por outro lado, foi projetado para processar enormes quantidades de dados em tempo real.

Em vez de acessar dados do armazenamento externo, o Spark copia os dados para a RAM antes de processá-los. Ele só grava os dados de volta no armazenamento externo depois de concluir uma tarefa específica. Escrever e ler da RAM é exponencialmente mais rápido do que fazer o mesmo com uma unidade externa. Além disso, o Spark reutiliza os dados recuperados para várias operações.